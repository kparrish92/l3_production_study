## Bayes?

# libs 

library(rstanarm)
library(bayesplot)
install.packages('LaplacesDemon')
library(LaplacesDemon)
library(tidyverse)


# Sim and tidy data
nparticipants = 80 
nquestions = 3 
k = 100

simdf = data.frame("participant" = rep(1:k,
                                       each = nquestions),
                   "question" = rep(1:nquestions,
                                    times = k))
simdf$L1 = rnorm(n = k*nquestions, m = 1978, sd = 400)
simdf$L2 = rnorm(n = k*nquestions, m = 1978, sd = 400)
simdf$L3 = rnorm(n = k*nquestions, m = 1353, sd = 84)

long = simdf %>%  
  pivot_longer(c(`L1`, `L2`, `L3`), names_to = "language", values_to = "vot")


# Fit model and convert to tibble; print summary
rope = sd(long$vot)*.5
fake_model = stan_glmer(vot ~ language + (1 + language | participant), data = long)

fake_model_post <- as_tibble(fake_model)
colnames(fake_model_post)[1] <- "intercept"

summary(fake_model)


# Plot forest plot and plot distribution of means 
fake_model_post %>% 
  dplyr::select(intercept, languageL2, languageL3) %>% 
  pivot_longer(cols = everything(), names_to = "parameter", 
               values_to = "estimate") %>% 
  ggplot(., aes(x = estimate, y = parameter)) + 
  geom_vline(xintercept = 0, lty = 3) +
  tidybayes::stat_halfeye(pch = 21, point_fill = "white", point_size = 3, 
                          .width = c(0.66, 0.95)) + 
  labs(title = "BDA", 
       subtitle = "Forest plot of population estimates", 
       caption = "Posterior means +/- 66% and 95% CI", 
       y = "Parameter", x = "Estimate") + geom_vline(xintercept = c(235, -235), linetype = "dashed") +
  scale_fill_manual(values = c("gray80", "skyblue"))


long %>% 
  ggplot(aes(x = vot, y = language)) + 
  tidybayes::stat_halfeye(pch = 21, point_fill = "white", point_size = 3, 
                          .width = c(0.66, 0.95))

hist(fake_model_post$languageL3)

hist(fake_model_post$languageL2)

hist(fake_model_post$intercept)

ESS(fake_model_post$languageL3)
ESS(fake_model_post$languageL2)
ESS(fake_model_post$intercept)



## ideas for by-item diss exp. 

# bayesian logistic regression - odds of producing L1 or L2 like VOT as a function of word bias (yes/no) 
# or - VOT as a function of bias by word. 

# sim logistic regression with bayesian analysis 


